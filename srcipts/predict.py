"""
æ­£ç¡®çš„SSv2è¯„ä¼°è„šæœ¬ï¼šç”¨å‰20å¸§é¢„æµ‹ç¬¬20å¸§çš„è¿›åº¦
"""

import os
import json
import torch
import argparse
import numpy as np
from tqdm import tqdm
from PIL import Image
import cv2
import matplotlib.pyplot as plt
from collections import defaultdict
import csv

from train_reg import get_transforms
from student import StudentModel


class CorrectSSv2Evaluator:
    """æ­£ç¡®çš„SSv2è¯„ä¼°å™¨ï¼šå‰20å¸§é¢„æµ‹ç¬¬20å¸§è¿›åº¦"""
    
    def __init__(self, model_path, num_frames=20, hidden_dim=512):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model_path: æ¨¡å‹checkpointè·¯å¾„
            num_frames: å¸§æ•°ï¼ˆå¿…é¡»æ˜¯20ï¼‰
            hidden_dim: éšè—ç»´åº¦ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        
        # åŠ è½½æ¨¡å‹
        self.model = StudentModel(
            num_frames=num_frames,
            hidden_dim=hidden_dim
        ).to(self.device)
        
        checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model'])
        self.model.eval()
        
        self.num_frames = num_frames
        self.transform = get_transforms(is_train=False)
        
        print(f"Model loaded successfully: {model_path}")
        print(f"  Frames: {num_frames}")
        print(f"  Hidden dimension: {hidden_dim}")
        print(f"  Task: Predict progress at frame {num_frames} using first {num_frames} frames")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.results = []
        self.metrics = defaultdict(list)
    
    def load_first_n_frames(self, video_path, n_frames):
        """
        åŠ è½½è§†é¢‘çš„å‰nå¸§
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            n_frames: è¦åŠ è½½çš„å¸§æ•°
            
        Returns:
            frames: åŠ è½½çš„å¸§åˆ—è¡¨
            success: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # å¦‚æœè§†é¢‘æ€»å¸§æ•°ä¸è¶³n_framesï¼Œè·³è¿‡
        if total_frames < n_frames:
            cap.release()
            return None, False
        
        frames = []
        for i in range(n_frames):
            ret, frame = cap.read()
            if not ret:
                # å¦‚æœä¸­é—´è¯»å–å¤±è´¥ï¼Œç”¨æœ€åä¸€å¸§å¡«å……
                if frames:
                    frame = frames[-1]
                else:
                    frame = np.zeros((224, 224, 3), dtype=np.uint8)
            
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = Image.fromarray(frame)
            frame = self.transform(frame)
            frames.append(frame)
        
        cap.release()
        return frames, True
    
    def evaluate_single_video(self, video_path, label, total_frames):
        """
        è¯„ä¼°å•ä¸ªè§†é¢‘
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            label: åŠ¨ä½œæ ‡ç­¾
            total_frames: æ€»å¸§æ•°
            
        Returns:
            result: é¢„æµ‹ç»“æœå­—å…¸
        """
        # è®¡ç®—çœŸå®è¿›åº¦ï¼šç¬¬20å¸§çš„è¿›åº¦ = 20 / æ€»å¸§æ•°
        target_progress = self.num_frames / total_frames if total_frames > 0 else 0
        
        # å¦‚æœè§†é¢‘å¤ªçŸ­ï¼Œè·³è¿‡
        if total_frames < self.num_frames:
            return None
        
        # åŠ è½½å‰20å¸§
        frames, success = self.load_first_n_frames(video_path, self.num_frames)
        if not success:
            return None
        
        # è½¬æ¢ä¸ºå¼ é‡
        video_tensor = torch.stack(frames).unsqueeze(0).to(self.device)  # (1, T, C, H, W)
        
        # é¢„æµ‹
        with torch.no_grad():
            pred_progress, _ = self.model(video_tensor, [label])
            pred_progress = pred_progress[0].item()
        
        # è®¡ç®—è¯¯å·®
        mae = abs(pred_progress - target_progress)
        
        result = {
            'video_id': os.path.basename(video_path).replace('.webm', ''),
            'video_path': video_path,
            'label': label,
            'total_frames': total_frames,
            'frames_used': self.num_frames,
            'pred_progress': pred_progress,
            'target_progress': target_progress,
            'mae': mae,
            'pred_percentage': int(pred_progress * 100),
            'target_percentage': int(target_progress * 100),
            'frame_ratio': f"{self.num_frames}/{total_frames}"
        }
        
        return result
    
    def evaluate_json(self, json_path, output_dir=None, max_videos=None):
        """
        è¯„ä¼°JSONæ–‡ä»¶ä¸­çš„æ‰€æœ‰è§†é¢‘
        
        Args:
            json_path: JSONæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            max_videos: æœ€å¤§è§†é¢‘æ•°ï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•ï¼‰
        """
        # åŠ è½½JSONæ•°æ®
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # é™åˆ¶è§†é¢‘æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if max_videos:
            data = data[:max_videos]
        
        print(f"Starting evaluation of {len(data)} videos...")
        print(f"  Task: Predict progress at frame {self.num_frames} using first {self.num_frames} frames")
        print("="*60)
        
        # è¯„ä¼°æ¯ä¸ªè§†é¢‘
        self.results = []
        skipped_videos = 0
        
        for video_info in tqdm(data, desc="Evaluating videos"):
            try:
                video_path = video_info['video_path']
                label = video_info['label']
                total_frames = video_info['num_frames']
                
                # è·³è¿‡å¸§æ•°å¤ªå°‘çš„è§†é¢‘
                if total_frames < self.num_frames:
                    skipped_videos += 1
                    continue
                
                # è¯„ä¼°å•ä¸ªè§†é¢‘
                result = self.evaluate_single_video(video_path, label, total_frames)
                
                if result:
                    self.results.append(result)
                    
                    # æ”¶é›†æŒ‡æ ‡
                    self.metrics['mae'].append(result['mae'])
                    self.metrics['pred_progress'].append(result['pred_progress'])
                    self.metrics['target_progress'].append(result['target_progress'])
                
            except Exception as e:
                print(f"\nâŒ Video evaluation failed: {e}")
                print(f"   Video: {video_info.get('video_path', 'N/A')}")
                continue
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        self.calculate_metrics()
        
        # è¾“å‡ºç»“æœ
        self.print_results(skipped_videos)
        
        # ä¿å­˜ç»“æœ
        if output_dir:
            self.save_results(output_dir)
        
        return self.results
    
    def calculate_metrics(self):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if not self.results:
            return
        
        mae_values = np.array(self.metrics['mae']) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”è¯¯å·®
        
        # è®¡ç®—ä¸‰ä¸ªå‡†ç¡®ç‡ç­‰çº§
        very_accurate = np.sum(mae_values <= 2) / len(mae_values) * 100
        nearly_accurate = np.sum(mae_values <= 8) / len(mae_values) * 100
        reasonably_accurate = np.sum(mae_values <= 15) / len(mae_values) * 100
        
        self.overall_metrics = {
            'num_videos': len(self.results),
            'mae_mean': np.mean(mae_values),
            'very_accurate_percentage': very_accurate,
            'nearly_accurate_percentage': nearly_accurate,
            'reasonably_accurate_percentage': reasonably_accurate,
            'accuracy_distribution': {
                'very_accurate_count': int(np.sum(mae_values <= 2)),
                'nearly_accurate_count': int(np.sum(mae_values <= 8)),
                'reasonably_accurate_count': int(np.sum(mae_values <= 15)),
                'total_count': len(mae_values)
            }
        }
        
        # ä¸ºå›¾è¡¨å‡†å¤‡æ•°æ®
        self.accuracy_counts = [
            int(np.sum(mae_values <= 2)),
            int(np.sum((mae_values > 2) & (mae_values <= 8))),
            int(np.sum((mae_values > 8) & (mae_values <= 15))),
            int(np.sum(mae_values > 15))
        ]
        
        self.accuracy_labels = ['Very Accurate (â‰¤2%)', 'Nearly Accurate (â‰¤8%)', 
                               'Reasonably Accurate (â‰¤15%)', 'Inaccurate (>15%)']
       
    def print_results(self, skipped_videos):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        if not hasattr(self, 'overall_metrics'):
            self.calculate_metrics()
        
        print("\n" + "="*60)
        print("SSv2 Evaluation Results")
        print("="*60)
        
        metrics = self.overall_metrics
        print(f"Valid videos: {metrics['num_videos']}")
        print(f"Skipped videos (too short): {skipped_videos}")
        print(f"\nMAE: {metrics['mae_mean']:.2f}%")
        print(f"\nAccuracy Metrics:")
        print(f"  Very Accurate (â‰¤2% error): {metrics['very_accurate_percentage']:.2f}%")
        print(f"  Nearly Accurate (â‰¤8% error): {metrics['nearly_accurate_percentage']:.2f}%")
        print(f"  Reasonably Accurate (â‰¤15% error): {metrics['reasonably_accurate_percentage']:.2f}%")
        
        # æ˜¾ç¤ºå‰10ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ
        print(f"\nğŸ” First 10 sample results:")
        print(f"{'#':<4} {'Video ID':<12} {'Frames':<8} {'Pred%':<8} {'True%':<8} {'MAE%':<8} {'Frame Ratio'}")
        print("-" * 70)
        for i, result in enumerate(self.results[:10]):
            print(f"{i+1:<4} {result['video_id']:<12} {result['total_frames']:<8.0f} "
                  f"{int(result['pred_percentage']):<8} {int(result['target_percentage']):<8} "
                  f"{result['mae']*100:<8.1f} {result['frame_ratio']}")
        
       
    def save_results(self, output_dir):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ä¿å­˜æŒ‡æ ‡åˆ°JSON
        json_path = os.path.join(output_dir, 'evaluation_metrics.json')
        with open(json_path, 'w') as f:
            json.dump({
                'overall_metrics': self.overall_metrics,
                'accuracy_distribution': self.overall_metrics['accuracy_distribution']
            }, f, indent=2)
        
        # 2. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.plot_results(output_dir)
        
        print(f"\nğŸ’¾ Results saved to: {output_dir}")
        print(f"  Evaluation metrics: {json_path}")
    
    def plot_results(self, output_dir):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            # 1. æ•£ç‚¹å›¾ï¼šé¢„æµ‹vsçœŸå®
            plt.figure(figsize=(10, 8))
            targets = np.array(self.metrics['target_progress']) * 100
            preds = np.array(self.metrics['pred_progress']) * 100
            
            plt.scatter(targets, preds, alpha=0.5, s=20)
            plt.plot([0, 100], [0, 100], 'r--', label='Ideal Prediction', alpha=0.7)
            plt.xlabel('True Progress (20/Total Frames) (%)')
            plt.ylabel('Predicted Progress (%)')
            plt.title(f'Frame {self.num_frames} Progress Prediction (MAE={self.overall_metrics["mae_mean"]:.2f}%)')
            plt.grid(True, alpha=0.3)
            plt.legend()
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'scatter_plot.png'), dpi=150)
            plt.close()
            
            # 2. è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
            plt.figure(figsize=(10, 6))
            errors = np.array(self.metrics['mae']) * 100
            plt.hist(errors, bins=30, edgecolor='black', alpha=0.7)
            plt.xlabel('Absolute Error (%)')
            plt.ylabel('Number of Samples')
            plt.title(f'Error Distribution (Mean={errors.mean():.2f}%)')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'error_distribution.png'), dpi=150)
            plt.close()
            
            # 3. å‡†ç¡®ç‡åˆ†å¸ƒå›¾
            plt.figure(figsize=(10, 7))
            
            # åˆ›å»ºå­å›¾
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
            
            # å·¦è¾¹ï¼šé¥¼å›¾
            colors = ['#4CAF50', '#8BC34A', '#FFC107', '#F44336']
            wedges, texts, autotexts = ax1.pie(
                self.accuracy_counts, 
                labels=self.accuracy_labels, 
                autopct='%1.1f%%',
                colors=colors,
                startangle=90
            )
            
            # ç¾åŒ–é¥¼å›¾æ–‡æœ¬
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
            
            ax1.set_title('Accuracy Distribution by Error Threshold')
            
            # å³è¾¹ï¼šæ¡å½¢å›¾
            percentages = [
                self.overall_metrics['very_accurate_percentage'],
                self.overall_metrics['nearly_accurate_percentage'] - self.overall_metrics['very_accurate_percentage'],
                self.overall_metrics['reasonably_accurate_percentage'] - self.overall_metrics['nearly_accurate_percentage'],
                100 - self.overall_metrics['reasonably_accurate_percentage']
            ]
            
            bars = ax2.bar(self.accuracy_labels, percentages, color=colors, alpha=0.8)
            ax2.set_ylabel('Percentage of Videos (%)')
            ax2.set_title('Cumulative Accuracy by Error Threshold')
            ax2.set_ylim(0, 100)
            ax2.grid(True, axis='y', alpha=0.3)
            
            # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, percentage in zip(bars, percentages):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{percentage:.1f}%', ha='center', va='bottom', fontsize=10)
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'accuracy_distribution.png'), dpi=150)
            plt.close()
            
            print(f"ğŸ“Š Visualizations saved: scatter_plot.png, error_distribution.png, accuracy_distribution.png")
            
        except Exception as e:
            print(f"âš ï¸  Error generating charts: {e}")


def main():
    parser = argparse.ArgumentParser(description='SSv2 Evaluation: Predict progress at frame 20 using first 20 frames')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--model_path', required=True, help='Path to trained model checkpoint')
    parser.add_argument('--test_json', required=True, help='Path to SSv2 format test JSON file')
    
    # æ¨¡å‹å‚æ•°ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    parser.add_argument('--num_frames', type=int, default=20, help='Number of frames to use')
    parser.add_argument('--hidden_dim', type=int, default=512, help='Hidden dimension')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--max_videos', type=int, default=None,
                       help='Maximum number of test videos (for quick testing)')
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument('--output_dir', type=str, default='./correct_ssv2_evaluation',
                       help='Output directory')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CorrectSSv2Evaluator(
        model_path=args.model_path,
        num_frames=args.num_frames,
        hidden_dim=args.hidden_dim
    )
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_json(
        json_path=args.test_json,
        output_dir=args.output_dir,
        max_videos=args.max_videos
    )
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    report_path = os.path.join(args.output_dir, 'summary.txt')
    with open(report_path, 'w') as f:
        f.write("="*60 + "\n")
        f.write("SSv2 Evaluation Summary Report\n")
        f.write("="*60 + "\n\n")
        f.write(f"Task: Predict progress at frame {args.num_frames} using first {args.num_frames} frames\n")
        f.write(f"Model: {args.model_path}\n")
        f.write(f"Test data: {args.test_json}\n")
        f.write(f"Valid videos: {evaluator.overall_metrics['num_videos']}\n")
        f.write(f"\nPerformance Metrics:\n")
        f.write(f"  MAE: {evaluator.overall_metrics['mae_mean']:.2f}%\n")
        f.write(f"  Very Accurate (â‰¤2% error): {evaluator.overall_metrics['very_accurate_percentage']:.2f}%\n")
        f.write(f"  Nearly Accurate (â‰¤8% error): {evaluator.overall_metrics['nearly_accurate_percentage']:.2f}%\n")
        f.write(f"  Reasonably Accurate (â‰¤15% error): {evaluator.overall_metrics['reasonably_accurate_percentage']:.2f}%\n")
    
    print(f"\nğŸ“„ Summary report saved: {report_path}")
    print("="*60)
    print("âœ… Evaluation completed!")


if __name__ == '__main__':
    main()